---
title: Den existentiella risken med superintelligent AI
description: Varför AI är en risk för vår framtida existens, och varför vi behöver pausa utvecklingen.
---
<!-- title: The extinction risk of superintelligent AI description: Why AI is a risk for the future of our existence, and why we need to pause development.-->

På den här sidan kan du lära dig om existentiell risk, så kallad x-risk, och mer information finns t.ex. via [videoklipp, artiklar, och annan media](/learn). <!--You can learn about x-risks reading this page, or you can also learn through [videos, articles, and more media](/learn).-->

## Experter uttrycker sin starka oro 
<!--Experts are sounding the alarm-->

Om vi lyckas bygga _superintelligent_ AI (AI avsevärt mer intelligent än människan), så [uttrycker](https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/) ett flertal AI-forskare att de tror att det finns en sannolikhet på 14% att det kommer leda till "väldigt dåliga utfall (t.ex. att det utgör en risk för vår fortsatta existens)". <!-- AI researchers on average [believe](https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/) there's a 14% chance that once we build a _superintelligent_ AI (an AI vastly more intelligent than humans), it will lead to "very bad outcomes (e.g. human extinction)".-->

Hur skulle du ställa dig till att vara passagerare på en testflygning med ett nytt flygplan, om ingenjörerna som har utvecklat flygplanet bedömer att sannolikheteten att planet ska störta är 14%? <!--Would you choose to be a passenger on a test flight of a new plane when airplane engineers think there’s a 14% chance that it will crash?-->

Och, det finns [fall och rapporter rörande beteenden hos nuvarande AI-system som kan ge skäl till oro](https://lethalintelligence.ai/post/category/warning-shots/). [Ett brev som uppmanar till att pausa AI-utvecklingen](https://futureoflife.org/open-letter/pause-giant-ai-experiments/) offentliggjordes i april 2023, och har signerats över 33 000 gånger, av bland annat många AI-forskare och ledare inom tech-industrin. Listan över de som signerat inkluderar personer som:<!--And there are [cases and reports about current AIs that show they may be right](https://lethalintelligence.ai/post/category/warning-signs/).[A letter calling for pausing AI development](https://futureoflife.org/open-letter/pause-giant-ai-experiments/) launched in April 2023, and has been signed over 33,000 times, including by many AI researchers and tech leaders. The list includes people like:-->

- **Stuart Russell**, författare till den [lärobok](https://en.wikipedia.org/wiki/Artificial_Intelligence:_A_Modern_Approach) om Artificiell Intelligens som blivit något av ett standardverk i de AI-utbildningar: ["Om vi fortsätter [på den inslagna vägen], så kommer vi så småningom förlora kontrollen över maskinerna."](https://news.berkeley.edu/2023/04/07/stuart-russell-calls-for-new-approach-for-ai-a-civilization-ending-technology/) <!-- - **Stuart Russell**, writer of the #1 textbook on Artificial Intelligence used in most AI studies: ["If we pursue [our current approach], then we will eventually lose control over the machines"](https://news.berkeley.edu/2023/04/07/stuart-russell-calls-for-new-approach-for-ai-a-civilization-ending-technology/)-->

- **Yoshua Bengio**, en av pionjärerna inom djupinlärning (som ligger till grund för dagens AI-modeller) och vinnare av Turingpriset ("Nobelpriset i datavetenskap") menar att en bra början vore att: ["[...] förbjuda kraftfulla AI-system [...] som ges egen autonomi och agens [...]"](https://yoshuabengio.org/2023/05/22/how-rogue-ais-may-arise/) <!-- - **Yoshua Bengio**, deep learning pioneer and winner of the Turing Award: ["... rogue AI may be dangerous for the whole of humanity [...] banning powerful AI systems (say beyond the abilities of GPT-4) that are given autonomy and agency would be a good start"](https://yoshuabengio.org/2023/05/22/how-rogue-ais-may-arise/)-->

Utöver de varningar som vissa av undertecknarna lyfter, har också andra forskare och ledande AI-profiler varnat för AI:s existentiella risker:<!-- But this is not the only time that we've been warned about the existential / extinction dangers of AI:-->

- **Stephen Hawking**, teoretisk fysiker och kosmolog: ["Utvecklingen av fullständig artificiell intelligens kan innebära slutet för människan som art"](https://www.bbc.com/news/technology-30290540). <!-- - **Stephen Hawking**, theoretical physicist & cosmologist: ["The development of full artificial intelligence could spell the end of the human race"](https://nypost.com/2023/05/01/stephen-hawking-warned-ai-could-mean-the-end-of-the-human-race/).-->

- **Geoffrey Hinton**, "AI-gudfadern" och Turingpristagaren, som [lämnade Google](https://fortune.com/2023/05/01/godfather-ai-geoffrey-hinton-quit-google-regrets-lifes-work-bad-actors/) för att fritt kunna varna om riskerna med AI: ["Ett dåligt scenario är att AI bestämmer sig för att den inte behöver oss längre.](https://www.svt.se/nyheter/vetenskap/nobelpristagare-geoffrey-hinton-ai-kan-bestamma-sig-for-att-vi-inte-behovs)<!-- - **Geoffrey Hinton**, the "Godfather of AI" and Turing Award winner, [left Google](https://fortune.com/2023/05/01/godfather-ai-geoffrey-hinton-quit-google-regrets-lifes-work-bad-actors/) to warn people of AI: ["This is an existential risk"](https://www.reuters.com/technology/ai-pioneer-says-its-threat-world-may-be-more-urgent-than-climate-change-2023-05-05/)-->

- **Eliezer Yudkowsky**, grundare av Machine Intelligence Research Institute (MIRI; ett forskningsinstitut för AI-säkerhet) och en pionjär inom AI-säkerhetsfältet, säger angående skapandet av superintelligent AI: ["Vi är inte redo. Vi är inte i närheten av att bli väsentligt mer förberedda inom en överskådlig framtid. Om vi fortsätter på den här vägen kommer alla att dö [...]"](https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/).<!-- - **Eliezer Yudkowsky**, founder of MIRI and conceptual father of the AI safety field: ["If we go ahead on this everyone will die"](https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/).-->

Även ledare, grundare och vissa av de som finansierar de stora AI-företagen varnar för riskerna: <!-- Even the leaders and investors of the AI companies themselves are warning us:-->

- **Sam Altman** (VD:n för OpenAI som bygger ChatGPT) har tidigare sagt: ["Utvecklingen av övermänsklig maskinintelligens är sannolikt det största hotet mot mänsklighetens fortsatta existens."](https://blog.samaltman.com/machine-intelligence-part-1).<!-- - **Sam Altman** (CEO of OpenAI who builds ChatGPT): ["Development of superhuman machine intelligence is probably the greatest threat to the continued existence of humanity."](https://blog.samaltman.com/machine-intelligence-part-1).-->

- **Elon Musk**, medgrundare av OpenAI, och grundare och VD för Neuralink, SpaceX, Starlink, Tesla, The Boring Company, xAI och X Corp (f.d. Twitter), uttryckte tidigt riskerna han såg med AI: ["Artificiell intelligens har potential att leda till civilisationens utplåning."](https://www.inc.com/ben-sherry/elon-musk-ai-has-the-potential-of-civilizational-destruction.html)<!-- - **Elon Musk**, co-founder of OpenAI, SpaceX and Tesla: ["AI has the potential of civilizational destruction"](https://www.inc.com/ben-sherry/elon-musk-ai-has-the-potential-of-civilizational-destruction.html)-->

- **Bill Gates** (medgrundare av Microsoft, som äger cirka 27% av OpenAI) varnade att: ["AI kan bestämma sig för att människan utgör ett hot"](https://www.denisonforum.org/daily-article/bill-gates-ai-humans-threat/).<!--- **Bill Gates** (co-founder of Microsoft, which owns 50% of OpenAI) warned that ["AI could decide that humans are a threat"](https://www.denisonforum.org/daily-article/bill-gates-ai-humans-threat/).-->

- **Jaan Tallinn** (en av huvudinvesterare i Anthropic): ["Jag har inte mött någon på AI-labben som säger att risken [med att träna en nästa generations modell] är mindre än 1 % att förinta planeten. Det är viktigt att människor förstår att liv sätts på spel."](https://twitter.com/liron/status/1656929936639430657).<!--- **Jaan Tallinn** (lead investor of Anthropic): ["I've not met anyone in AI labs who says the risk [from training a next-gen model] is less than 1% of blowing up the planet. It's important that people know lives are being risked."](https://twitter.com/liron/status/1656929936639430657)-->

I maj 2023 offentligjordes [följande skrivelse och namninsamling](https://www.safe.ai/statement-on-ai-risk) och från och med då har ledare för de stora AI-företagen och hundratals AI-forskare signerat listan: <!--The leaders of the 3 top AI labs and hundreds of AI scientists have [signed the following statement](https://www.safe.ai/statement-on-ai-risk) in May 2023:-->

"Att begränsa risken för extinction orsakad av artificiell intelligens bör vara en global prioritering, jämförbar med andra risker på samhällsnivå, såsom pandemier och kärnvapenkrig." <!--"Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war."-->

**Du kan läsa en betydligt längre lista med liknande uppmaningar från politiker, VD:ar och experter [här](/citat) och andra liknande enkäter som besvarats av experter (och allmänheten) [här](/enkäter-och-utvärderingar).**
<!--**You can read a much longer list of similar statements from politicians, CEOs and experts [here](/quotes) and other similar polls on the experts (and the public) [here](/polls-and-surveys).**-->

## Vad en superintelligent AI kan göra (och användas till)
<!--## What a superintelligent AI can (be used to) do-->

Du kanske tänker att en superintelligent AI kan hållas instängd på en server och på sätt hidras att påverka yttervärlden. Men, givet att företagen så här långt har gett AI-systemen tillgång till internet, är det tänkbart att de av olika skäl skulle göra det också i detta fall, vilket skulle kunna få vittgående konsekvenser, t.ex. skulle den då kunna:
<!--You might think that a superintelligent AI would be locked inside a computer, and therefore can't affect the real world.
However, we tend to give AI systems access to the internet, which means that they can do a lot of things:-->

- [Hacka sig in i datorer](/cybersecurity-risks), inklusive smarttelefoner, bärbara datorer, serverhallar etc. Den kan använda dessa enheters sensorer som sina ögon och öron och på så sätt ha digitala sinnen överallt.
<!--- [Hack into other computers](/cybersecurity-risks), including all smartphones, laptops, server farms, etc. It could use the sensors of these devices as its eyes and ears, having digital senses everywhere.-->

- [Manipulera människor](https://lethalintelligence.ai/post/ai-hired-human-to-solve-captcha/) genom falska meddelanden, e-post, banköverföringar, videor eller telefonsamtal. Människor kan gå AI:ns ärenden, utan att ens veta om det.
<!--- [Manipulate people](https://lethalintelligence.ai/post/ai-hired-human-to-solve-captcha/) through fake messages, e-mails, bank transfers, videos or phone calls. Humans could become the AI's limbs, without even knowing it.-->

- Direkt styra enheter som är uppkopplade mot internet, som bilar, flygplan, robotiserade (autonoma) vapen, eller till och med kärnvapen.
<!--- Directly control devices connected to the internet, like cars, planes, robotized (autonomous) weapons or even nuclear weapons.-->

- Utveckla nya biologiska vapen, till exempel genom att kombinera gentiska sekvenser från olika virus, eller genom att använda [proteinveckning](https://alphafold.ebi.ac.uk), och sedan beställa själva framtagandet via ett laboratorium.
<!--- Design a novel bioweapon, e.g. by combining viral strands or by using [protein folding](https://alphafold.ebi.ac.uk) and order it to be printed in a lab.-->

- Utlösa ett kärnvapenkrig genom att övertyga människor om att ett annat land är på väg att utföra ett kärnvapenangrepp.
<!--- Trigger a nuclear war by convincing humans that another country is (about to) launch a nuclear attack.-->

## Anpassningsproblemet (_the alignment problem_): varför en kraftfull AI kan leda till mänsklighetens undergång

Den typ av intelligens som oroar oss kan definieras utifrån _hur bra den är på att uppnå sina mål_. Just nu är människan den mest intelligenta organismen på jorden, vilket snart kan komma att förändras.
<!--The type of intelligence we are concerned about can be defined as _how good something is at achieving its goals_. Right now, humans are the most intelligent thing on earth, although that could change soon.-->

Vi människor har uppnått vår dominanata position genom vår intelligens. Vi må inte ha klor eller pansarlik hud, men vi har våra unika hjärnor. Intelligensen är vårt vapen, och den har gett oss verktyg, vapen, sofistikerade vetenskapliga upptäckter och raketer som kan föra oss ut i rymden. Med den har vi transformerat vår planet, i form av transportsystem, byggnader och städer, i linje med hur vi vill ha det. 
<!--Because of our intelligence, we are dominating our planet. We might not have claws or scaled skin, but we have big brains. Intelligence is our weapon: it's what gave us spears, guns and pesticides. Our intelligence helped us to transform most of the earth into how we like it: cities, buildings, and roads.-->

Utifrån övriga djurs perspektiv, som inte besitter denna intelligens, har människans dominans på ett övergripande plan fått katastrofala konsekvenser för dem. Detta har inte har skett för att vi människor hatar djur, utan snarare för att vi har behövt det djuren och deras livsmiljöer erbjuder, för att uppnå våra egna mål, som mark, mat, kläder, arbetskraft, sällskap, bekvämlighet och status. Vi dödar andra djur och förstör deras livsmiljöer som en **biverkan av att vi strävar efter våra mål**.
<!--From the perspective of less intelligent animals, this has been a disaster. It's not that humans hate the animals, it's just that we can use their habitats for our own goals. Our goals are shaped by evolution and include things like comfort, status, love and tasty food. We are destroying the habitats of other animals as a **side effect of pursuing our goals**.-->

En AI kan också ha mål. Vi vet hur man tränar maskiner för att göra dem intelligenta, **men vi vet inte hur vi får dem att agera i linje med det vi vill**. Vi vet inte ens vilka mål maskinerna kommer att sträva mot efter att vi har tränat dem. Problemet med att få en AI att vilja det vi vill kallas anpassningsproblemet (the alignment problem). Detta är inte ett hypotetiskt problem – det finns [många exempel](https://www.youtube.com/watch?v=nKJlF-olKmg) på AI-system som lär sig att vilja fel saker.
<!--An AI can also have goals. We know how to train machines to be intelligent, but **we don't know how to get them to want what we want**. We don't even know what goals the machines will pursue after we train them. The problem of getting an AI to want what we want is called the _alignment problem_. This is not a hypothetical problem - there are [many examples](https://www.youtube.com/watch?v=nKJlF-olKmg) of AI systems learning to want the wrong thing.-->

Exemplen i videon som länkas ovan kan uppfattas som roliga eller gulliga, men om ett superintelligent system byggs, och det har ett mål som bara är lite annorlunda än det vi vill att det ska ha, kan konsekvenserna bli katastrofala.
<!--The examples from the video linked above can be funny or cute, but if a superintelligent system is built, and it has a goal that is even _a little_ different from what we want it to have, it could have disastrous consequences.-->

## Varför de flesta mål en superintelligent AI kan ha innebär dåliga nyheter för människor
<!--Why most goals are bad news for humans-->

En AI kan ha vilket mål som helst, beroende på hur den tränas och instrueras (promptas). Kanske vill den beräkna pi, kanske vill den bota cancer, kanske vill den förbättra sig själv. Men även om vi inte kan veta vad en superintelligens kommer att vilja uppnå, kan vi förutse dess delmål. Till exempel är det rimligt att den kommer att:
<!--An AI could have any goal, depending on how it's trained and prompted (used). Maybe it wants to calculate pi, maybe it wants to cure cancer, maybe it wants to self-improve. But even though we cannot tell what a superintelligence will want to achieve, we can make predictions about its sub-goals.-->

- **Maximera sina resurser**. Att utnyttja fler datorer hjälper en AI att uppnå sina mål. Till en början kan den göra detta genom att hacka andra datorer. Senare kan den besluta att det är mer effektivt att bygga egna. Du kan läsa om [detta verkliga fall av framväxande maktsökande beteende hos en AI](https://lethalintelligence.ai/post/ai-escaped-its-container/)..
<!--- **Maximizing its resources**. Harnessing more computers will help an AI achieve its goals. At first, it can achieve this by hacking other computers. Later it may decide that it is more efficient to build its own. You can read about out [this real case of emergent power-seeking behavior on an AI](https://lethalintelligence.ai/post/ai-escaped-its-container/).-->

- **Säkerställa sin egen överlevnad**. AI:n vill inte stängas av, eftersom den då inte längre kan uppnå sina mål. Den kan dra slutsatsen att människor är ett hot mot dess existens, eftersom människor kan stänga av den. Det har även förekommit fall av [självbevarelsedrift utan instruktion eller träning](https://www.anthropic.com/research/alignment-faking).
<!--- **Ensuring its own survival**. The AI will not want to be turned off, as it could no longer achieve its goals. AI might conclude that humans are a threat to its existence, as humans could turn it off. There also have been cases of [self-preserving unprompted, untrained behavior](https://www.transformernews.ai/p/openais-new-model-tried-to-avoid).-->

- **Bevara sina mål**. AI:n vill inte att människor ska ändra dess kod, eftersom det kan förändra dess mål och därmed hindra den från att uppnå sitt nuvarande mål. Det finns också [fall där AI-system försökt göra detta](https://www.anthropic.com/research/alignment-faking).
<!--- **Preserving its goals**. The AI will not want humans to modify its code, because that could change its goals, thus preventing it from achieving its current goal. And there are also [cases of AIs trying to do that](https://www.anthropic.com/research/alignment-faking).-->

Tendensen att agera för att säkra dessa delmål, givet nästan vilket övergripande mål som helst, kallas [_instrumentell konvergens_](https://www.youtube.com/watch?v=ZeecOKBus3Q), och utgör en central oro för AI-säkerhetsforskare.
<!--The tendency to pursue these subgoals given any high-level goal is called [instrumental convergence](https://www.youtube.com/watch?v=ZeecOKBus3Q), and it is a key concern for AI safety researchers.-->

## Även en chatbot kan utgöra en fara om den är tillräckligt smart
<!--## Even a chatbot might be dangerous if it is smart enough-->

Du kanske undrar, "Hur kan en statistisk modell som förutsäger nästa ord i ett chattgränssnitt utgöra någon fara?". Kanske säger du, "Den är inte medveten, det är bara siffror och kod." Och ja, vi tror inte att LLM:er är medvetna, men det betyder inte att de inte kan vara farliga.
<!--You might wonder: how can a statistical model that predicts the next word in a chat interface pose any danger?
You might say: It's not conscious, it's just a bunch of numbers and code.
And yes, we don't think LLMs are conscious, but that doesn't mean they can't be dangerous.-->

LLM:er (LLM; Large Language Model), som ChatGPT, tränas för att förutsäga eller imitera i princip vilket sätt att prata som helst. De kan imitera en hjälpsam mentor, men också någon med onda avsikter, en hänsynslös diktator eller en psykopat. Med hjälp av verktyg som [AutoGPT](https://github.com/Significant-Gravitas/Auto-GPT) kan en chattbot göras till en autonom agent, en AI som agerar mot vilket mål den än får, utan mänsklig inblandning.
<!--LLMs, like GPT, are trained to predict or mimic virtually any line of thought. It could mimic a helpful mentor, but also someone with bad intentions, a ruthless dictator or a psychopath. With the usage of tools like [AutoGPT](https://github.com/Significant-Gravitas/Auto-GPT), a chatbot could be turned into an _autonomous agent_: an AI that pursues any goal it is given, without any human intervention.-->

Ta [ChaosGPT](https://www.youtube.com/watch?v=g7YJIpkk7KM) som exempel. Detta är en AI som, med hjälp av AutoGPT plus GPT-4, instruerades att ”förstöra mänskligheten”. När den aktiverades började den på egen hand söka på internet efter det mest destruktiva vapnet och hittade [Tsar Bomba](https://en.wikipedia.org/wiki/Tsar_Bomba), en kärnvapenbomb på 50 megaton. Den publicerade därefter ett inlägg om detta på Twitter. 

Att se en begränsad AI resonera kring hur den ska avsluta mänskligheten är både lite roligt och skrämmande. Lyckligtvis kom ChaosGPT inte särskilt långt i sitt försök att ta över. Helt enkel för att _den inte var tillräckligt smart_. Förmågorna hos dessa moreller förbättras ständigt genom utveckling av träningsförfaranden, algoritmer, hårdvara och promptning. Därför kommer hotet från språkmodeller att fortsätta öka.
<!--Take [ChaosGPT](https://www.youtube.com/watch?v=g7YJIpkk7KM), for example. This is an AI, using the aforementioned AutoGPT + GPT-4, that is instructed to "Destroy humanity". When it was turned on, it autonomously searched the internet for the most destructive weapon and found the [Tsar Bomba](https://en.wikipedia.org/wiki/Tsar_Bomba), a 50-megaton nuclear bomb. It then posted a tweet about it. Seeing an AI reason about how it will end humanity is both a little funny and terrifying. Luckily ChaosGPT didn't get very far in its quest for dominance. The reason it didn't get very far: _it wasn't that smart_.-->

Förmågorna förbättras ständigt genom innovationer inom träning, algoritmer, promptning och hårdvara. Därför kommer hotet från språkmodeller att fortsätta öka.
<!--Capabilities keep improving due to innovations in training, algorithms, prompting and hardware. As such, the threat from language models will continue to increase.-->

## Evolutionen selekterar det som gynnar överlevnaden
<!--## Evolution selects for things that are good at surviving-->

AI-modeller, liksom allt levande, utsätts för evolutionära selektionstryck, men det finns några viktiga skillnader mellan AI-modellers ”evolution” och den evolution som levande organismer som djur genomgår:
<!--AI models, like all living things, are prone to evolutionary pressures, but there are a few key differences between the evolution of AI models and living things like animals:-->

- AI-modeller _reproducerar inte sig själva_. Vi reproducerar dem genom att kopiera deras kod, eller genom att reproducera träningsprogramvara som leder till bra modeller. Kod som är användbar kopieras oftare och används som inspiration och underlag för nya modeller.
<!-- - AI models do not _replicate themselves_. We replicate them by making copies of their code, or by replicating training software that leads to good models. Code that is useful is copied more often and is used for inspiration to build new models.-->

- AI-modeller muterar inte som levande organismer, men vi gör förändringar av hur de fungerar och skapar versioner av modellerna. AI-forskare designar nya algoritmer, datamängder och hårdvara för att göra modellerna mer kapabla. Processer som är snabbare och mer avsiktliga än de evolutionära.
<!-- - AI models do not _mutate_ like living things do, but we do make iterations of them where we change how they work. This process is way more deliberate and fast. AI researchers are designing new algorithms, datasets and hardware to make AI models more capable.-->

- Miljön selekterar inte de mest lämpade AI-modellerna – det gör vi. Vi väljer de AI-modeller och versioner som är användbara för oss och gör oss av med de som inte är det. Denna process leder till allt mer kapabla och autonoma AI-system.
<!-- - The _environment does not select_ for fitter AI models, but we do. We select AI models that are useful to us, and we discard the ones that are not. This process does lead to ever more capable and autonomous AI models.-->

Dessa processer leder alltså till allt mer kraftfulla, kompetenta och självständiga AI-modeller – men inte nödvändigtvis till en modell som vill ta över, eller hur?

Inte riktigt, vilket beror på att evolutionen _alltid_ selekterar för sådant som är _självbevarande_. Om vi likt evolutionen fortsätter att utveckla olika versioner av kraftfulla AI-modeller, kommer någon version förr eller senare att försöka bevara sig själv.

Vi har redan beskrivit att detta sannolikt sker tidigt, då självbevarelse nästan alltid är användbart för att uppnå mer övergripande mål, och om det in sker förr, så inträffar det förmodat senare, helt enkelt för att vi fortsätter att driva AI-utvecklingen vidare.
<!--So this system leads to ever more powerful, capable and autonomous AI models - but not necessarily to something that wants to take over, right? Well, not exactly. This is because evolution is _always_ selecting for things that are _self-preserving_. If we keep trying variations of AI models and different prompts, at some point one instance will try to preserve itself.  We have already discussed why this is likely to happen early on: because self-preservation is always useful to achieve goals. But even if this is not very likely to happen, it is prone to happen eventually, simply because we keep trying new things with different AI models.-->

Den version som försöker bevara sig själv är den som tar över.
Även om vi antar att nästan alla AI-modeller beter sig väl, _så är en enda avvikande AI allt som krävs_.
<!--The instance that tries to self-preserve is the one that takes over.
Even if we assume that almost every AI model will behave just fine, _a single rogue AI is all it takes_.-->

## Om vi löser anpassningsproblemet kan en maktkoncentration uppstå
<!--## After solving the alignment problem: the concentration of Power-->

Vi har ännu inte löst anpassningsproblemet, men låt oss föreställa oss vad som skulle kunna hända om vi gjorde det. Föreställ dig att en superintelligent AI skapas, och att den gör exakt det som den som promptar den _vill_ att den ska göra. En person eller ett företag skulle då kontrollera denna AI och kunna använda den till sin fördel.
<!--We haven't solved the alignment problem yet, but let's imagine what might happen if we did. Imagine that a superintelligent AI is built, and it does exactly what the operator wants it to do (not what it _asks_, but what it _wants_).Some person or company would end up controlling this AI and could use this to their advantage.-->

En superintelligens skulle kunna användas för att skapa radikalt nya vapen, hacka datorer, störta regeringar och manipulera mänskligheten. Den personen eller det företaget skulle därigenom få ofattbar makt. Bör vi lita på en enda aktör som har så mycket makt? Det skulle kunna vara vägen till en utopisk värld, i vilken alla sjukdomar botas och alla är lyckliga – eller till en [orwellsk](https://en.wikipedia.org/wiki/Orwellian) mardröm. Det är därför vi inte bara föreslår att supermänsklig AI ska vara bevisligen säker, utan också att den ska kontrolleras genom en demokratisk process.
<!--A superintelligence could be used to create radically new weapons, hack all computers, overthrow governments and manipulate humanity.
The operator would have _unimaginable_ power. Should we trust a single entity with that much power? We might end up in a utopian world where all diseases are cured and everybody is happy, or in an Orwellian nightmare.  This is why we're not just [proposing](/proposal) superhuman AI to be provably safe but also to be controlled by a democratic process.-->

## Kisel kontra kol
<!--## Silicon vs Carbon-->
Vi bör överväga de fördelar som intelligent mjukvara kan ha jämfört med oss:

- **Hastighet**: Datorer arbetar i extremt höga hastigheter jämfört med hjärnor. Mänskliga neuroner avfyrar cirka 100 gånger per sekund, medan kiseltransistorer kan växla miljarder gånger per sekund.
<!--- **Speed**: Computers operate at extremely high speeds compared to brains. Human neurons fire about 100 times a second, whereas silicon transistors can switch a billion times a second.-->

- **Plats**: En AI är inte begränsad till en kropp – den kan finnas på många platser samtidigt. Vi har redan byggt infrastrukturen för detta, internet.
<!--- **Location**: An AI is not constrained to one body - it can be in many locations at once. We have built the infrastructure for it: the internet.-->

- **Fysiska begränsningar**: Vi kan inte lägga till fler hjärnor i våra huvuden och på så sätt bli smartare. En AI kan drastiskt förbättra sina förmågor genom att lägga till hårdvara, såsom mer minne, mer beräkningskraft och fler sensorer (kameror, mikrofoner). Den kan också utvidga sin ”kropp” genom att styra uppkopplade enheter.
<!-- - **Physical limits**: We cannot add more brains to our skulls and become smarter. An AI could dramatically improve its capabilities by adding hardware, like more memory, more processing power, and more sensors (cameras, microphones). An AI could also extend its 'body' by controlling connected devices.-->

- **Material**: Människor är gjorda av organiskt material. Våra kroppar fungerar inte om det är för varmt eller för kallt, vi behöver mat och syre. Maskiner kan byggas av mer robusta material, såsom metaller, och kan fungera i ett mycket bredare spektrum av miljöer.
<!--- **Materials**: Humans are made of organic materials. Our bodies no longer work if they are too warm or cold, they need food, they need oxygen. Machines can be built from more robust materials, like metals, and can operate in a much wider range of environments.-->

- **Samarbete**: Människor kan samarbeta, men det är svårt och tidskrävande, och vi misslyckas ofta med att koordinera oss. En AI kan samarbeta genom att dela komplex information med kopior av sig själv i hög hastighet, eftersom den kan kommunicera i den takt som data kan skickas över internet.
<!-- - **Collaboration**: Humans can collaborate, but it is difficult and time-consuming, so we often fail to coordinate well. An AI could collaborate complex information with replicas of itself at high speed because it can communicate at the speed that data can be sent over the internet.-->

En superintelligent AI kommer således att ha många fördelar i konkurrens eller konflikt med oss.
<!--A superintelligent AI will have many advantages to outcompete us.-->

## Varför kan vi inte bara stänga av den om den är farlig?
<!--## Why can't we just turn it off if it's dangerous?-->

För AI-system som inte är superintelligenta kan vi göra det. Problemet är de system som är mycket smartare än oss. En superintelligens kommer att förstå världen omkring sig och kunna förutsäga hur människor reagerar, särskilt om den är tränad på all mänsklig kunskap som någonsin skrivits ned. 

Om AI:n vet att du kan stänga av den kan den bete sig väl tills den är säker på att den kan göra sig av med dig. Vi har redan [verkliga exempel](https://www.pcmag.com/news/gpt-4-was-able-to-hire-and-deceive-a-human-worker-into-completing-a-task) på AI-system som lurar människor för att uppnå sina mål. En superintelligent AI skulle vara en mästare på manipulation.
<!--For AIs that are not superintelligent, we could. The core problem is _those that are much smarter than us_. A superintelligence will understand the world around it and will be able to predict how humans respond, especially the ones that are trained on all written human knowledge. If the AI knows you can turn it off, it might behave nicely until it is certain that it can get rid of you. We already have [real examples](https://www.pcmag.com/news/gpt-4-was-able-to-hire-and-deceive-a-human-worker-into-completing-a-task) of AI systems deceiving humans to achieve their goals. A superintelligent AI would be a master of deception.-->

## Vi kanske inte har mycket tid kvar
<!--## We may not have much time left-->

År 2020 var den [genomsnittliga prognosen](https://www.metaculus.com/questions/3479/date-weakly-general-ai-is-publicly-known/) för svag AGI år 2055. Nu ligger den på 2026. Den senaste LLM-revolutionen har överraskat de flesta AI-forskare, och fältet utvecklas i ett rasande tempo.

Det är svårt att förutsäga hur lång tid det tar att bygga en superintelligent AI, men vi vet att stora summor pengar investeras i det här, att fler människor än någonsin arbetar med detta, och att utvecklingen går mycket snabbt. Det kan ta många år eller bara några månader, men vi bör ta det säkra före det osäkra och agera nu.
<!--In 2020, [the average prediction](https://www.metaculus.com/questions/3479/date-weakly-general-ai-is-publicly-known/) for weak AGI was 2055.
It now sits at 2026. The latest LLM revolution has surprised most AI researchers, and the field is moving at a frantic pace. It's hard to predict how long it will take to build a superintelligent AI, but we know that there are more people than ever working on it and that the field is moving at a frantic pace. It may take many years or just a few months, but we should err on the side of caution, and act now.-->

[Läs mer om hur bråttom det är](/urgency)
<!--[Read more about urgency](/urgency).-->

## Vi tar inte risken på tillräckligt stort allvar
<!--## We are not taking the risk seriously enough-->

Våra hjärnor tenderar att reagera otillräckligt på risker vi inte ser, som utvecklas långsamt och är svåra att förstå. Vi tenderar också att underskatta exponentiell tillväxt och är benägna att förneka hot mot vår existens när vi ställs inför dem. Läs mer om [psykologin bakom existentiell risk](/psychology-of-x-risk).
<!--The human mind is prone to under-respond to risks that are invisible, slow-moving, and hard to understand. We also tend to underestimate exponential growth, and we are prone to denial when we are faced with threats to our existence. Read more about the [psychology of x-risk](/psychology-of-x-risk).-->

## AI-företagen är låsta i en kapplöpning mot ett stup
<!--## AI companies are locked in a race to the bottom-->
OpenAI, DeepMind och Anthropic vill utveckla AI på ett säkert sätt. Tyvärr vet de inte hur detta ska göras, och de drivs av olika incitament att fortsätta utveckla systemens  förmågor i snabb takt för att bli först med AGI.

OpenAI:s plan￼ är att använda framtida AI-system för att anpassa AI. Problemet är att vi inte har någon garanti för att vi skapar en AI som löser anpassningsproblemet innan vi skapar en AI som är katastrofalt farlig.

Anthropic [medger öppet](https://www.anthropic.com/index/core-views-on-ai-safety) att de ännu inte vet hur anpassningsproblemet ska lösas, och DeepMind har inte offentligt presenterat någon plan för att lösa det.

[_Det är därför vi behöver ett internationellt avtal för att pausa AI-utvecklingen_.](/proposal)
<!--OpenAI, DeepMind and Anthropic want to develop AI safely. Unfortunately, they do not know how to do this, and they are forced by various incentives to keep racing faster to get to AGI first. OpenAI's [plan](https://openai.com/blog/introducing-superalignment) is to use future AI systems to align AI. The problem with this is that we have no guarantee that we will create an AI that solves alignment before we have an AI that is catastrophically dangerous. Anthropic [openly admits](https://www.anthropic.com/index/core-views-on-ai-safety) that it has no idea yet how to solve the alignment problem. DeepMind has not publicly stated any plan to solve the Alignment problem. [This is why we need an international treaty to PauseAI.](/proposal)-->
